\section{Models and Algorithms}

In this section, we describe the Bayesian hierarchical additive model that \texttt{BHAM} implements. The key idea is to impose the two-part spike-and-slab LASSO prior \cite{guo2022_GAM} on each additive function in generalized models or Cox proportional hazards models. For the additive function $B_j(X_j)$ of the $j$th variable, the proposed two-part spike-and-slab LASSO prior consists of a spike-and-slab LASSO prior for the linear space coefficient $\beta_j$ and a modified group spike-and-slab LASSO prior for the nonlinear space coefficients $\beta_{jk}^\tp, k = 1,..., K_j$, \begin{align}
  \beta_{j} | \gamma_{j},s_0,s_1 &\sim (1-\gamma_{j}) DE(0, s_0) + \gamma_{j} DE(0, s_1)\nonumber \\
  \beta^\tp_{jk} | \gamma^\tp_{j},s_0,s_1 &\simiid (1-\gamma_{j}^\tp) DE(0, s_0) + \gamma_{j}^\tp DE(0, s_1), k=1,\dots, K_j.
\end{align} To note, the model matrix of each additive function undergoes a reparameterization process in advance, which eigendecomposes the smoothing penalty matrix to isolate the linear and nonlinear spaces of the additive function \citep{wood2017}. The reparameterization greatly reduces the complexity to formulate the sparsity-smoothness penalty \citep{meier2009} and allows different shrinkage on the two spaces. The shrinkage on the linear space manages the variable selection, while the shrinkage on the nonlinear space emphasizes the adequate smoothing of nonlinear effect interpolation. In addition, the isolation of linear and nonlinear spaces motivates the bi-level functional selection, i.e. the selection of additive functions and the selection of nonlinear effects. In the proposed prior, each additive function has two indicators $\gamma_{j}$ and $\gamma^\tp_{j}$, controlling the linear and nonlinear component selection. Effect hierarchy was implemented via the conditional priors of $\gamma^\tp_{j}$ to ensure the the linear component is more likely to be selected than the nonlinear components. \begin{align}
&\gamma_{j} | \theta_j \sim Bin(1, \theta_j) & & 
&\gamma_{j}^\tp | \gamma_{j}, \theta_j \sim Bin(1, \gamma_{j}\theta_j).
\end{align} We further impose a beta prior on the inclusion probability parameter $\theta_j$ to allow locally adaptive shrinkage. For simplicity, $\theta_j$ follows a uniform(0, 1) prior. Compared to previous spike-and-slab priors \citep{scheipl2012, bai2021} for additive functions, the proposed prior provides three advantages. First of all, the proposed prior allows bi-level functional selection instead of an "all-in-all-out" approach for variable selection. Secondly, the proposed prior offers a natural selection procedure by shrinking unnecessary coefficients to exactly 0, contrasting to soft-thresholding the inclusion probability. Last but not least, the proposed prior is easily applicable to model different types of outcomes, including time-to-event outcomes via Cox proportional hazards models.

To fit the proposed models in a efficient and scalable fashion, we develop the EM-coordinate descent algorithm. The EM-coordinant descent algorithm estimates maximum a posteriori of coefficients by optimizing the log joint posterior density function. The algorithm formulates the spike-and-slab LASSO prior as a double exponential distribution with a conditional scale parameter. It further leverages the relationship between double exponential prior and $l_1$ penalty and expresses the log joint posterior density function as the summation of a $l_1$ penalized likelihood function ( and $l_1$ penalized partial likelihood function for Cox proportional hazards models) and log beta posterior densities. Because the nuisance parameters $\bs \gamma$ are unknown, we instead optimize the conditional expectation of log joint posterior density function via an EM procedure \citep{dempster1977}. In each iterations of the EM procedure, we update the expectation of the log joint posterior density function with respect to the nuisance parameters, calculate the penalties based on the estimation from previous iteration, and optimize the penalized likelihood and the posterior density with coordinate descent algorithm and closed-form calculation for the coefficients. The process iterates until convergence. Cross-validation is used to choose the optimal model. We defer to \cite{guo2022_GAM, guo2022_Cox} for more detail on the GAM algorithm and the additive Cox model algorithm.
