\section{Features}

In this section, we demonstrate how to fit Bayesian hierarchical additive model with two-part spike-and-slab LASSO prior, and introduce the model tuning, diagnostic and other utility functions for visualize additive functions, bi-level selection.

In this section, we describe to the users the workflow for of fitting a high-dimensional additive model with the two-part spike-and-slab LASSO prior  in `BHAM`. Specifically, we introduce how to 1) prepare the high-dimensional design matrix for fitting the proposed model, 2) fit generalized additive model, 3) Model tuning and performance assessment, and 4) visualize the bi-level variable selection.


\subsection{Installation}
To install the latest development version of `BHAM` package from **GitHub**, type the following command in R console:

```{r eval = FALSE}
if (!require(devtools)) install.packages("devtools")
if(!require(BHAM)) devtools::install_github("boyiguo1/BHAM", build_vignettes = FALSE)
```

You can also set `build_vignettes=TRUE` but this will slow down the installation drastically (the vignettes can always be accessed online anytime at [boyiguo1.github.io/BHAM/articles](https://boyiguo1.github.io/BHAM/articles)).

\subsection{Preliminaries}
We use a simulated data set to demonstrate our package. The data generating mechanism is motivated by Bai (citation) and programmed in the function `sim_Bai`: we assume there are $p=10$ predictors where the first four predictors have effects on the outcome (see functions below), and the rest of predictors don't, i.e $B_j(x_j) = 0, j = 5, \dots, p$. [TODO: Insert functions here on what the equations are]. 
With the data generating mechanism, we simulate two datasets with the binary outcome from Bernoulli trials with logit link function. To note, the function `sim_Bai` can also simulate Gaussian and Poisson outcomes using the same data generating mechanism. The sample sizes of these two datasets are 500 and 1000 for training and testing respectively. The following code section creates the training and testing datasets.
```{r}
library(BHAM)
set.seed(1) ## simulate some data... 
n_train <- 500
n_test <- 1000
p <- 10
# Train Data
train_dat <- sim_Bai(n_train, p)
dat <- train_dat$dat %>% data.frame

# Test Data
test_tmp <- sim_Bai(n_test, p)
test_dat <- test_tmp$dat %>% data.frame
```
The first ten observation of the data set look like below
```{r echo = FALSE}
head(dat, 10)
```


\subsection{Set up Design Matrix of additive functions}
Given the raw data, we would like to translate the additive functions to the their matrix form. The challenge here is to allow a customizable and convenient way to specify the high-dimensional model. Our solution here is to use a data frame to accomodate each predictor in the raw data set, and allows each predictor have their spline function specified. There are three columns for this formulat specification data frame, including `Var` `Func`, `Args`. The `Var` column hosts the variable name; the `Func` column hosts the spline function following the commonly used generalized additive model \texttt{mgcv}; the `Args` column hosts the detail specification of the spline function. The data frame can be constructed manually for low-dimensional settings and also be manipulated easily when the number of spline components grows to tens or hundreds. See the examples below.

```{r}
# Low-dimensional setting
mgcv_df <- dplyr::tribble(
  ~Var, ~Func, ~Args,
  "X1",  "s",     "bs='cr', k=5",
  "X2",  "s",     NA,
  "X3",  "s",    "",
)

# High-dimensional setting
mgcv_df <- data.frame(
  Var = setdiff(names(dat), "y"),
  Func = "s",
  Args ="bs='cr', k=7"
)
```

After having the model specification data frame, the next task is to construct the overall design matrix. We provide a function `construct_smooth_data` to construct the design matrix for each predictor according to their spline specification iteratively, and binding all design matrices together with a systematic naming convention. The linear component of the spline function is named with the suffix `.null` and the nonlinear components are named with the suffix `.pen`. In `construct_smooth_data`, we take three steps of matrix manipulation via the `smoothCon` from the package `mgcv`: 1) linear constraints, 2) eigendecomposition of the smoothing matrix $S$ to isolate linear and nonlinear spaces, 3) scaling of the design matrix such that the coefficients are on the same scale. As we use `mgcv::smoothCon` to decode the spline specification, we carry over the ability to work with user-defined spline functions as long as it follows `mgcv` standard. 

The `construct_smooth_data` function have two arguments, the model specification data frame and the raw data, and return the finalized design matrix `data` and the smooth specification functions `Smooth` which will be used to construct the design matrix of the new datasets for the prediction purpose.

```{r}
train_sm_dat <- BHAM::construct_smooth_data(mgcv_df, dat)
train_smooth <- train_sm_dat$Smooth
train_smooth_data <- train_sm_dat$data
```

## Fitting the Bayesian Hierarchical model
The model fitting  function is similar to 

```{r eval = F}
bham_mdl <- bamlasso(x = train_smooth_data, y = dat$y, family = "binomial", group = make_group(names(train_smooth_data)))
```


## Tuning via Cross-validation
```{r eval = F}
s0_seq <- seq(0.005, 0.1, 0.01)
cv_res <- tune.bgam(bham_mdl, nfolds = 5, s0= s0_seq, verbose = FALSE)
```

## Fit Optimal model
```{r eval = F}
s0_min <- cv_res$s0[which.min(cv_res$deviance)]
bham_final <- bamlasso(x = train_smooth_data, y = dat$y, family = "binomial", group = make_group(names(train_smooth_data)),
                       ss = c(s0_min, 0.5))
```

## Variable selecrtion
```{r eval = F}
bamlasso_vs_part <- bamlasso_var_selection(bham_final)
```

### Curv Plotting
```{r eval = F}
plot_smooth_term(bham_final, "x1", train_smooth,
                     min = min(dat[, "x1"])-0.1,
                     max = max(dat[, "x1"]) + 0.1)
```

## Prediction

Due to the reparameterization in the design matrix, it creates complication with setting up the design matrix for the test data. Hence, we write an wrapping function for `PredictMat` from `mgcv`.

```{r eval = F}
train_smooth <- train_sm_dat$Smooth
test_sm_dat <- make_predict_dat(train_sm_dat$Smooth, dat = test_dat)
```


```{r eval = F}
if(!require("devtools")) install.packages("devtools")
if(!require("BhGLM")) devtools::install_github("nyiuab/BhGLM ")

BhGLM::measure.bh(bham_final, test_sm_dat, test_dat$y)

# pred <- predict.glm(.mdl$mdl, newdata = test_sm_dat, type = "response")
# measure.glm(test_dat$y, pred, family = fam_fun$family) 
```
