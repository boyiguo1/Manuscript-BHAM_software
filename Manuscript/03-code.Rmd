\section{Features}

In this section, we demonstrate how to fit Bayesian hierarchical additive model with two-part spike-and-slab LASSO prior, and introduce the model tuning, diagnostic and other utility functions for visualize additive functions, bi-level selection.

In this section, we describe to the users the workflow for of fitting a high-dimensional additive model with the two-part spike-and-slab LASSO prior  in `BHAM`. Specifically, we introduce how to 1) prepare the high-dimensional design matrix for fitting the proposed model, 2) fit generalized additive model, 3) Model tuning and performance assessment, and 4) visualize the bi-level variable selection.


\subsection{Installation}
To install the latest development version of `BHAM` package from **GitHub**, type the following command in R console:

```{r eval = FALSE}
if (!require(devtools)) {
install.packages("devtools")
}
devtools::install_github("boyiguo1/BHAM", build_vignettes = FALSE)
```

You can also set `build_vignettes=TRUE` but this will slow down the installation drastically (the vignettes can always be accessed online anytime at [boyiguo1.github.io/BHAM/articles](https://boyiguo1.github.io/BHAM/articles)).

\subsection{Preliminaries}
We write a function that follows the data generating equation from Bai (citation). The outcome distribution could come from one of Gaussian, Poisson, or Binomial distributions. Please see (function link) here. Note here, the data output from the the simulation function is "raw" data. In order to have the final design matrix for analysis, you need to set up the spline functions of your preference via a data frame (see example in ), and use the function `construct_smooth_data`.

```{r}
library(BHAM)
set.seed(1) ## simulate some data... 
n_train <- 500
n_test <- 1000
p <- 10
# Train Data
train_dat <- sim_Bai(n_train, p)
dat <- train_dat$dat %>% data.frame
theta <- train_dat$theta

# Test Data
test_tmp <- sim_Bai(n_test, p)
test_dat <- test_tmp$dat %>% data.frame
test_theta <- test_tmp$theta
```



\subsection{Make Design Matrix}
#### Set up Spline function
Since our models are primarily designed for high-dimensional applications. Hence, we are mindful about how to formulate the regression equation. Our solution here is to use a data frame which contains the necessary components of a spline function, in the format of `Var` `Func`, `Args`, which later passed to a parser function, which will assemble the regression equation for you. We believe such automation process, implemented in `construct_smooth_data`, will ease the task load to set up the regression formula, when the number of spline components grows to tens or hundreds.

```{r}
# Low-dimensional setting
mgcv_df <- dplyr::tribble(
  ~Var, ~Func, ~Args,
  "X1",  "s",     "bs='cr', k=5",
  "X2",  "s",     NA,
  "X3",  "s",    "",
)

# High-dimensional setting
mgcv_df <- data.frame(
  Var = setdiff(names(dat), "y"),
  Func = "s",
  Args ="bs='cr', k=7"
)
```

#### Make Design Matrix and Prediction Matrix
We use `smoothCon` from the package `mgcv` to construct the . Hence, our `construct_smooth_data` have the potential to work with user-defined spline functions as long as it follows `mgcv` standard. 

In our `construct_smooth_data`, we have taken multiple reparameterization steps via `smoothCon`: 1) linear constraints, 2) eigen decomposition of the smoothing matrix $S$ to isolate null and penalized spaces, 3) scaling of the design matrix to have unit variance for each column of the data set, i.e. bases of the design matrix.


```{r}
train_sm_dat <- BHAM::construct_smooth_data(mgcv_df, dat)
train_smooth <- train_sm_dat$Smooth
train_smooth_data <- train_sm_dat$data
```

Due to the reparameterization in the design matrix, it creates complication with setting up the design matrix for the test data. Hence, we write an wrapping function for `PredictMat` from `mgcv`.

```{r}
train_smooth <- train_sm_dat$Smooth
test_sm_dat <- make_predict_dat(train_sm_dat$Smooth, dat = test_dat)
```



## Fitting the Bayesian Hierarchical model
The model fitting  function is similar to 

```{r}
bham_mdl <- bamlasso(x = train_smooth_data, y = dat$y, family = "binomial", group = make_group(names(train_smooth_data)))
```


## Tuning via Cross-validation
```{r}
s0_seq <- seq(0.005, 0.1, 0.01)
cv_res <- tune.bgam(bham_mdl, nfolds = 5, s0= s0_seq, verbose = FALSE)
```

## Fit Optimal model
```{r}
s0_min <- cv_res$s0[which.min(cv_res$deviance)]
bham_final <- bamlasso(x = train_smooth_data, y = dat$y, family = "binomial", group = make_group(names(train_smooth_data)),
                       ss = c(s0_min, 0.5))
```

## Variable selecrtion
```{r}
bamlasso_vs_part <- bamlasso_var_selection(bham_final)
```

### Curv Plotting
```{r}
plot_smooth_term(bham_final, "x1", train_smooth,
                     min = min(dat[, "x1"])-0.1,
                     max = max(dat[, "x1"]) + 0.1)
```

## Prediction

```{r}
if(!require("devtools")) install.packages("devtools")
if(!require("BhGLM")) devtools::install_github("nyiuab/BhGLM ")

BhGLM::measure.bh(bham_final, test_sm_dat, test_dat$y)

# pred <- predict.glm(.mdl$mdl, newdata = test_sm_dat, type = "response")
# measure.glm(test_dat$y, pred, family = fam_fun$family) 
```
